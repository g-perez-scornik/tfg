\chapter{Tangency}

At this point we have enough of the basic theory in place to begin doing actual \emph{geometry} on ``manifolds'' (microlinear objects).

\section{The tangent bundle}

Let \( M \) be a microlinear object and \( p\in M \). We make the following

\begin{defn}
  A tangent vector to \( M \) at \( p \) is a mapping 
  \begin{equation*}
    t:D\to M
  \end{equation*}
  such that \( t(0)=p \).
\end{defn}

We call the collection of all such tangent vectors \( T_pM \), the tangent space at \( p \) of \( M \). In resemblence to classical geometry, we of course expect each \( T_pM \) to have a vector space structure (and to justify calling them \emph{vectors} in the first place). It will not be so, since \( R \) is not a field in the classical sense, but we will prove that each \( T_pM \) is a Euclidean \( R \)-module. Let us begin by defining scalar multiplication:

\begin{equation*}
  R\times T_pM \to T_pM \atop (\lambda,t)\mapsto \lambda t
\end{equation*}

where the map \( \lambda t: D\to M \) is defined by

\begin{equation*}
  (\lambda t)(d) = t(\lambda d)
\end{equation*}

As for addition, consider two tangent vectors at \( p \), \( t_1,t_2:D\to M \). We now use that \( M \) is microlinear. Recall from earlier that the diagram below is a quasi pushout:

\begin{equation}
  \xymatrix{
    {\{0\}} \ar[d]_0 \ar[r]^0   & D \ar[d]^{i_2} \\
    D \ar[r]_{i_1}              & D(2)
  }
  \label{dg:pushoutcopy}
\end{equation}

Since \( M \) is microlinear, it perceives \ref{dg:pushoutcopy} as a pushout. As we studied in the case of \( R \), this means that the maps \( D(2)\to M \) are in bijection with pairs of maps \( D\to M \) that are equal at \( 0 \). This is the case with \( t_1 \) and \( t_2 \), since \( t_1(0)=p=t_2(0) \). Thus, there exists a unique map, which we call

\begin{equation*}
  s_{t_1,t_2}:D(2)\to M
\end{equation*}

allowing us to define

\begin{equation*}
  t_1+t_2:d\mapsto s_{t_1,t_2}(d,d)
\end{equation*}

Finally, let us define \( 0\in T_pM \) as the constand map \( 0(d)=p \,\forall d\in D \), and the additive opposite of a vector \( t \) as \( -t \), given by \( (-t)(d) = t(-d) \).

\begin{proposition}
  Let \( p\in M \). With the operations defined above, \( T_pM \) is an \( R \)-module.
\end{proposition}

\begin{proof}
  \begin{enumerate}
    \item Addition is commutative. Let \( t_1,t_2\in T_pM \). Again, this defines
      \begin{equation*}
	s_{t_1,t_2}:D(2)\to M
      \end{equation*}
      as the unique map satisfying \( s_{t_1,t_2}(d_1,0) = t_1(d_1) \) and \( s_{t_1,t_2}(0,d_2) = t_2(d_2) \). On the other hand, \( t_1 \) and \( t_2 \) also define the map
      \begin{equation*}
	s_{t_2,t_1}:D(2)\to M
      \end{equation*}
      unique among maps satisfying \( s_{t_2,t_1}(d_2,0) = t_2(d_2) \) and \( s_{t_2,t_1}(0,d_1) = t_1(d_1) \). Thererore, we have that for all \( (d_1,d_2)\in D(2) \)
      \begin{equation*}
	s_{t_1,t_2}(d_1,d_2) = s_{t_2,t_1}(d_2,d_1)
      \end{equation*}
      In particular, \( s_{t_1,t_2} \) and \( s_{t_2,t_1} \) are equal on the diagonal \( \left\{ (d,d)\mid d\in D \right\}\subset D(2) \). So, by definition of \( t_1+t_2 \) and \( t_2+t_1 \), these two are equal.

    \item Addition is associative. This requires a generalization of proposition \ref{prop:pullback}, that is, that \( R \) perceives the diagram
      \begin{equation}
	\xymatrix{
	  {\{0\}} \ar[d]_0 \ar[r]^0   & D(q) \ar[d]^{i_2} \\
	  D(p) \ar[r]_{i_1}              & D(p+q)
	}
	\label{dg:pushoutgen}
      \end{equation}
      as a pushout, for any \( p,q\geq 1 \). Here the maps \( i_1,i_2 \) are
      \begin{align*}
	i_1(d_1,\dots,d_p) &= (d_1,\dots,d_p,0,\dots,0) \\
	i_2(d_1,\dots,d_q) &= (0,\dots,0,d_1,\dots,d_q)
      \end{align*}
      The proof is nearly identical to the case we have examined (\( p=q=1 \)), one can refer to \cite[p. 48]{lav96} for details. That \( R \) perceives \ref{dg:pushoutgen} as a pushout means that \( M \) does as well, \( M \) being microlinear. In turn, this implies that the diagram
      \begin{equation*}
	\xymatrix{
	  M                & M^{D(2)} \ar[l]_{M^0} \\
	  M^D \ar[u]^{M^0} & M^{D(3)} \ar[u]_{M^{i_2}} \ar[l]^{M^{i_1}}
	}
      \end{equation*}
      is a pullback. That is, any function
      \begin{equation*}
	g:D(3)\to M \atop (d_1,d_2,d_3)\mapsto g(d_1,d_2,d_3)
      \end{equation*}
      is uniquely determined by the functions
      \begin{equation*}
	g_1:D\to M \atop d_1\mapsto g(d_1,0,0)
      \end{equation*}
      and
      \begin{equation*}
	g_{23}:D(2)\to M \atop (d_2,d_3) \mapsto g(0,d_2,d_3)
      \end{equation*}
      Now, by applying the characterization of maps \( D(2)\to M \) on \( g_{23} \), we obtain that \( g \) is the unique map satisfying \( g(d_1,0,0)=g_1(d_1) \), \( g(0,d_2,0)=g_{23}(d_2,0) \), and \( g(0,0,d)=g_{23}(0,d_3) \) for all \( (d_1,d_2,d_3)\in D(3) \). In other words, any map
      \begin{equation*}
	g:D(3)\to M
      \end{equation*}
      is uniquely determined by three maps \( g_1,g_2,g_3:D\to M \) with \( g_1(0)=g_2(0)=g_3(0) \), and such that
      \begin{align*}
	g_1(d_1) &= g(d_1,0,0) \\
	g_2(d_2) &= g(0,d_2,0) \\
	g_3(d_3) &= g(0,0,d_3)
      \end{align*}
      With that said, let \( t_1,t_2,t_3\in T_pM \) be three tangent vectors. As before, \( t_1 \) and \( t_2 \) determine a unique map
      \begin{equation*}
	s_{t_1,t_2}:D(2)\to M
      \end{equation*}
      So that, together with \( t_3 \), they determine a map \( g_l:D(3)\to M \) given by
      \begin{align*}
	g_l(d_1,0,0) &= s_{t_1,t_2}(d_1,0) \\
	g_l(0,d_2,0) &= s_{t_1,t_2}(0,d_2) \\
	g_l(0,0,d_3) &= t_3(d_3)
      \end{align*}
      Likewise, we obtain a map \( g_r \) by
      \begin{align*}
	g_r(d_1,0,0) &= t_1(d_1)           \\
	g_r(0,d_2,0) &= s_{t_2,t_3}(d_2,0) \\
	g_r(0,0,d_3) &= s_{t_2,t_3}(0,d_3)
      \end{align*}
      But these are the same map, since they both satisfy
      \begin{align*}
	g_l(d_1,0,0) &= g_r(d_1,0,0) = t_1(d_1) \\
	g_l(0,d_2,0) &= g_r(0,d_2,0) = t_2(d_2) \\
	g_l(0,0,d_3) &= g_r(0,0,d_3) = t_3(d_3)
      \end{align*}
      Now note that
      \begin{align*}
	((t_1+t_2)+t_3)(d) &= s_{t_1+t_2,t_3}(d,d) = g_l(d,d,d) \\
	(t_1+(t_2+t_3))(d) &= s_{t_1,t_2+t_3}(d,d) = g_r(d,d,d) 
      \end{align*}
      by which we conclude that \( ((t_1+t_2)+t_3) = (t_1+(t_2+t_3)) \).

    \item The map 0 is the additive identity. Let \( t:D\to M \) be a tangent vector at \( p \), and \( 0:D\to M \) be the constant \( p \). The identities
      \begin{align*}
	s_{t,0}(d,0) &= t(d) \\
	s_{t,0}(0,d) &= 0(d) = p
      \end{align*}
      uniquely determine \( s_{t,0}:D(2)\to M \). But the function \( t\circ \pi_1:D(2)\to M \), where \( \pi_2:D(2)\to D \) is the projection onto the second coordinate, also satisfies those equations, making \( s_{t,0} = t\circ \pi_2 \). Therefore,
      \begin{equation*}
	(t+0)(d) = s_{t,0}(d,d) = (t\circ \pi_2)(d,d) = t(d)
      \end{equation*}

    \item The opposite of a vector, as defined, satisfies \( t + (-t) = 0 \). The function \( s_{t,-t}:D(2)\to M \) is unique such that \( s_{t,-t}(d,0) = t(d) \) and \( s_{t,-t}(0,d) = (-t)(d) = t(-d) \). Again, we explicitly exhibit the function
      \begin{equation*}
	f:D(2)\to M \atop (d_1,d_2)\mapsto t(d_1-d_2)
      \end{equation*}
      and \( f \) satisfies the same equations, making it equal to \( s_{t,-t} \). Therefore, for any \( d\in D \)
      \begin{equation*}
	(t+(-t))(d) = s_{t,-t}(d,d) = f(d,d) = t(d-d) = t(0) = p
      \end{equation*}

    \item We have the identities
      \begin{enumerate}
	\item\label{a} \( (\alpha + \beta)t = \alpha t + \beta t \)
	\item\label{b} \( \alpha(t_1+t_2) = \alpha t_1 + \alpha t_2\)
	\item\label{c} \( \alpha (\beta t) = (\alpha \beta) t\)
	\item\label{d} \( 1\cdot t = t \)
      \end{enumerate}
      With \( \alpha,\beta\in R \) and \( t,t_1,t_2\in T_pM \). The tangent vector \( \alpha t + \beta t \) is the unique map \( s_{\alpha t, \beta t}:D(2)\to M \) satisfying
      \begin{align*}
	s_{\alpha t, \beta t}(d_1,0) &= (\alpha t)(d_1)= t(\alpha d_1) \\
	s_{\alpha t, \beta t}(0,d_2) &= (\beta t)(d_2)= t(\beta d_2)
      \end{align*}
      On the other hand, the map \( c \), defined by
      \begin{equation*}
       c(d_1,d_2) = t(\alpha d_1 + \beta d_2)
      \end{equation*}
      satisfies the equations, by which \( s_{\alpha t, \beta t} = c\). So,
      \begin{equation*}
	(\alpha t + \beta t)(d) = c(d,d) = t(\alpha d + \beta d) = t((\alpha + \beta)d) = (\alpha + \beta)t(d)
      \end{equation*}
      by definition. That proves \ref{a}. To prove \ref{b} we make an analogous argument, this time considering
      \begin{equation*}
	c(d_1,d_2) = s_{t_1,t_2}(\alpha d_1, \alpha d_2)
      \end{equation*}
      leaving \ref{c} and \ref{d}, which are immediate.
  \end{enumerate}
  This concludes the proof that \( T_pM \) is an \( R \)-module. We will now prove that it is Euclidean. Recall that this means that, for any map \( \varphi:D\to T_pM \), there should be a unique vector \( t\in T_pM \) such that, for all \( d\in D \)
  \begin{equation*}
    \varphi(d) = \varphi(0) + d\cdot t
  \end{equation*}
  This will again be a consequence of microlinearity. Define
  \begin{equation*}
    \tau : D\times D\to M \atop (d_1,d_2)\mapsto (\varphi(d_1)-\varphi(0))(d_2)
  \end{equation*}
  Note that \( \varphi(d_1)-\varphi(0) \) is a tangent vector at \( p \), so that
  \begin{equation*}
    \tau(d_1,0) = \tau(0,d_2) = \tau(0,0) = p
  \end{equation*}
  for all \( d_1,d_2\in D \). We'll now need another lemma concerning a quasi colimit.

  \begin{lemma}
    M perceives the diagram
    \begin{equation}
      \xymatrix{
	D \ar@<1em>[r]^{i_1} \ar[r]^{i_2} \ar@<-1em>[r]^{0} & D\times D \ar[r]^{\mu} & D
      }
      \label{dg:3coequ}
    \end{equation}
    \label{lm:3coequ}
    as a colimit (a ``triple coequalizer'', if one wishes). Here the maps \( i_1,i_2 \) are defined by \( i_1(d)=(d,0), i_2(d)=(0,d) \), \( 0 \) is the zero map, and \( \mu \) is the multiplication of elements of \( R \), restricted to elements of \( D \).
  \end{lemma}

  In other words, the diagram
  \begin{equation*}
    \xymatrix{
      M^D & M^{D\times D} \ar@<1ex>[l]_{M^{i_1}} \ar[l]_{M^{i_2}} \ar@<-1ex>[l]_{M^{0}} & M^D \ar[l]_{M^{\mu}} 
    }
  \end{equation*}
  is a limit (a triple equalizer). That is, if \( g\in M^{D\times D} \) is such that
  \begin{equation}
    g(d_1,0)=g(0,d_2)=g(0,0) \quad \forall d_1,d_2\in D
    \label{eq:3equ}
  \end{equation}
  then there exists a unique mapping 
  \begin{equation*}
    t:D\to M
  \end{equation*}
  such that
  \begin{equation*}
    g(d_1,d_2) = (t\circ \mu)(d_1,d_2) = t(d_1d_2)
  \end{equation*}
  
  This is all we need, since the map \( \tau \) verifies the equations \ref{eq:3equ}. Therefore there exists a unique \( t:D\to M \) with
  \begin{equation*}
    \tau(d_1,d_2)=t(d_1d_2) \quad \forall d_1,d_2\in D
  \end{equation*}
  In other words for all \( d_1,d_2 \),
  \begin{equation*}
    (\varphi(d_1) - \varphi(0))(d_2) = t(d_1d_2) = (d_1t)(d_2)
  \end{equation*}
  where the last equality is by definition of scalar multiplication of tangent vectors. The above holds in particular for all \( d_2\in D \), so that there is an equality of maps
  \begin{equation*}
    \varphi(d_1) = \varphi(0)+d_1t
  \end{equation*}
\end{proof}.

The proof of lemma \ref{lm:3coequ} is fairly straightforward. Since \( M \) is microlinear, it amounts to proving that diagram \ref{dg:3coequ} is a quasi colimit.

\begin{proof}[Proof of lemma \ref{lm:3coequ}]
  The diagram of Weil algebras
  \begin{equation}
    \xymatrix{
      \Q[X]/(X^2) & \Q[X,Y]/(X^2,Y^2) \ar@<3ex>[l]_{f_1} \ar[l]_{f_2} \ar@<-3ex>[l]_{f_3} & \Q[X]/(X^2) \ar[l]_{m} 
    }
    \label{dg:3equWalg}
  \end{equation}
  is a limit. Here the functions \( m,f_1,f_2,f_3 \) are defined by
  \begin{align*}
    f_1: & \begin{array}{c@{\hspace{0.3em}}l} X & \mapsto X \\ Y & \mapsto 0 \end{array} \\[4ex]
    f_2: & \begin{array}{c@{\hspace{0.3em}}l} X & \mapsto 0 \\ Y & \mapsto X \end{array} \\[4ex]
    f_3: & \begin{array}{c@{\hspace{0.3em}}l} X & \mapsto 0 \\ Y & \mapsto 0 \end{array} \\[4ex]
    m:   & \begin{array}{c@{\hspace{0.3em}}l} X & \mapsto XY                 \end{array} 
  \end{align*}
  To begin with, the diagram obviously commutes. Now, let \( A \) be any object, and \( g:A\to \Q[X,Y]/(X^2,Y^2) \) a map such that \( f_i\circ g = f_j\circ g \) for any \( i,j \) (\( g \) makes a similar diagram commute). Let \( a\in A \). Its image under \( g \) is an element of \( \Q[X,Y]/(X^2,Y^2) \), and such can be written, modulo \( (X^2,Y^2) \) as
  \begin{equation*}
    g(a) = c_{00} + c_{10}X + c_{01}Y + c_{11}XY
  \end{equation*}
  The condition that \( g \) commutes with the \( f_i \) force \( c_{10}=c_{01}=0 \). Define
  \begin{equation*}
    h(a) = c_{00} + c_{11}X
  \end{equation*}
  By varying \( a \) this defines a map \( h:A\to \Q[X]/(X^2) \). Since \( c_{00},c_{11} \) are unique modulo \( (X^2,Y^2) \), the map \( h \) is the unique map such that the following diagram commutes:
  \begin{equation*}
    \xymatrix{
      \Q[X]/(X^2) & \Q[X,Y]/(X^2,Y^2) \ar@<3ex>[l]_{f_1} \ar[l]_{f_2} \ar@<-3ex>[l]_{f_3} & \Q[X]/(X^2) \ar[l]_{m} \\
      &                                                                                   & A \ar[lu]^g \ar@{.>}[u]^h
    }
  \end{equation*}
  This concludes that diagram \ref{dg:3equWalg} is a limit. It is left to the reader to check that diagram \ref{dg:3coequ} is the result of applying the \( \spec_R \) functor to \ref{dg:3equWalg}, making it a quasi colimit. Since \( M \) was microlinear, the lemma is proven.  
\end{proof}

The natural next step is to define the differential (or derivative, or tangent map, etc.) of a mapping between two microlinear objects. Let \( M,N \) be microlinear, and \( f:M\to N \) a map. Let \( p\in M \). We define:
\begin{equation*}
  df_p : T_pM\to T_{f(p)}N
\end{equation*}
by
\begin{equation*}
  df_p(t) = f\circ t
\end{equation*}

As we should expect, this map is linear. Since \( T_pM \) is Euclidean, by proposition \ref{prop:homg} it suffices to see that it is homogeneous. For \( d\in D \), we have
\begin{equation*} 
  df_p(\alpha t)(d) = (f\circ (\alpha t))(d) = f(t(\alpha d)) = (f\circ t)(\alpha d) = (\alpha \cdot df_p(t))(d)
\end{equation*}

Another desirable property is that if \( V \) is a Euclidean \( R \)-module, it is canonically isomorphic to \( T_pV \) at every \( p\in V \). This is easy to see by presenting the isomorphism explicitly. Define
\begin{equation*}
  \lambda_p:V\to T_pV
\end{equation*}
by sending \( v\in V \) to the map
\begin{equation*}
  d\mapsto p+dv
\end{equation*}
This is a bijection since, by virtue of \( V \) being Euclidean, every map \( t:D\to V \) is characterized by a unique \( b\in V \) such that
\begin{equation*}
  d\mapsto t(0)+db
\end{equation*}
In \( T_pV \), each \( t(0) \) is equal to \( p \), so the inverse of \( \lambda_p \) is \( \lambda_p^{-1}(t) = b \). The map \( \lambda_p \) is obviously homogeneous, so again by proposition \ref{prop:homg} it is linear. As a short exercise, one can prove that, under this identification, \( df_p \) corresponds to \( df(p) \) (definition \ref{def:df(a)}) for maps of Euclidean \( R \)-modules \( f:V\to E \).

\section{Vector bundles; the tangent bundle}

\begin{defn}
  Let \( \pi:E\to M \) be a mapping of microlinear objects. We say that \( \pi \) is a \emph{(resp. Euclidean) vector bundle} if each fiber \( \pi_p = \pi^{-1}(\{p\}) \) of \( \pi \) is a (resp. Euclidean) \( R \)-module.
\end{defn}

As a prominent example, we have

\begin{defn}
  Let \( M \) be a microlinear object. The \emph{tangent bundle} on \( M \) is given by
  \begin{equation*}
    \pi:M^D\to M \atop t\mapsto t(0)
  \end{equation*}
\end{defn}

The previous map indeed defines a Euclidean vector bundle. First, \( M^D \) is microlinear by proposition \ref{prop:expmicro}, and second each fiber is just \( T_pM \), which we have just seen to be a Euclidean \( R \)-module.

We have defined vector bundles, so we should define what it means to be a morphism of vector bundles. If
\begin{align*}
  \pi_1:E_1\to M_1 \\
  \pi_2:E_2\to M_2 \\
\end{align*}
are two vector bundles, then a pair \( (\varphi,f) \) is said to be a morphism of the vector bundles \( \pi_1,\pi_2 \) if the following diagram commutes:
\begin{equation*}
  \xymatrix{
    E_1 \ar[d]_{\pi_1} \ar[r]^\varphi & E_2 \ar[d]^{\pi_2} \\
    M_1                \ar[r]_f       & M_2
  }
\end{equation*}
Equivalently, \( \varphi \) takes the fiber at \( p\in M_1 \) to the fiber at \( f(p)\in M_2 \).

Again, an important example is provided by the tangent bundle to a microlinear object. If \( M,N \) are microlinear objects, and \( f:M\to N \) is a map, then \( (f^D,f) \) is a morphism of the tangent bundle at \( M \) to the tangent bundle at \( N \), as evidenced by the commutative diagram
\begin{equation*}
  \xymatrix{
    M^D \ar[d]_{\pi_M} \ar[r]^{f^D} & N^D \ar[d]^{\pi_N} \\
    M                  \ar[r]_f     & N
  }
\end{equation*}
Where \( \pi_M,\pi_N \) are the projections defining the tangent bundles of \( M,N \), respectively. If we define \( \mlin \) to be the category of microlinear objects (with morphisms regular maps), and \( \vbun \) the category of vector bundles (with morphisms of vector bundles as previously defined), then the association of each microlinear object to its tangent bundle defines a functor
\begin{equation*}
  T:\mlin\to\vbun
\end{equation*}
with
\begin{align*}
  TM &= M^D     \\
  Tf &= (f^D,f) \\
\end{align*}

which we call the \emph{tangent functor}. Note that fiberwise, this is exactly the association of each object to its tangent space at a point \( p \), and each map \( f \) to its differential \( df_p \).

\section{Vector Fields}

As in classical differential geometry, we define a \emph{section} of a vector bundle to be a right inverse to the projection map of the bundle. That is, if \( \pi:E\to M \) is a vector bundle, then a section of \( \pi \) is a map \( s:M\to E \) such that \( \pi\circ s = \id_M \). We will write \( \Gamma(\pi) \) for the set of sections of \( \pi \).

Let \( \pi:E\to M \) be a vector bundle. It's immediate that \( \Gamma(\pi) \) comes equipped with an \( R \)-module structure, by the \( R \)-module structure on each fiber: define \( (s_1+s_2)(p) = s_1(p)+s_2(p) \), and so on (where \( s_1,s_2 \) are sections). Furthermore we have the following proposition.

\begin{proposition}
  The set \( \Gamma(\pi) \) is microlinear, and if \( \pi \) is a Euclidean vector bundle, \( \Gamma(\pi) \) is a Euclidean \( R \)-module.
\end{proposition}

\begin{proof}
  The objects \( E \) and \( M \) are microlinear by definition of a vector bundle, and by proposition \ref{prop:expmicro} so are \( E^M, M^M \) microlinear. Now observe that the diagram
  \begin{equation*}
    \xymatrix{
      {\Gamma(\pi)\,\,} \ar@{>->}[r] & E^M \ar@<1ex>[r]^{\varphi} \ar@<-1ex>[r]_{I} & M^M 
    }
  \end{equation*}
  is an equalizer, where the first arrow is just the inclusion of \( \Gamma(\pi) \) into \( E^M \), \( \varphi(s) = \pi\circ s \), and \( I(s) = \id_M \). Thus \( \Gamma(\pi) \) is a limit of microlinear objects, meaning it is microlinear by proposition \ref{prop:limmicro}. Now suppose that \( \pi \) is a Euclidean vector bundle, and let \( f:D\to \Gamma(\pi) \) be a map. For fixed \( p\in M \), the Eucldiean \( R \)-module structure on \( \pi_p \) gives that there exists a unique \( b(p)\in \pi_p \) (we make explicit the dependence on \( p \)) such that for all \( d\in D \)
  \begin{equation*}
    f(d)(p) = f(0)(p) + d\cdot b(p)
  \end{equation*}
  Varying \( p \) thus gives a section \( b:M\to E \) (it is a section, since \( b(p)\in \pi_p = \pi^{-1}(p) \)) which is unique such that
  \begin{equation*}
    f(d) = f(0) + d\cdot b
  \end{equation*}
  and so \( \Gamma(\pi) \) is Euclidean.
\end{proof}

We should also make the observation that \( \Gamma(\pi) \) is a module over \( R^M \), as well. For \( f:M\to R \) and \( s\in \Gamma(\pi) \) define

\begin{equation*}
  (f\cdot s)(p)=f(p)\cdot s(p)
\end{equation*}

We can now define what a vector field is, in a much expected way.

\begin{defn}
  Let \( M \) be a microlinear object. A \emph{vector field} on \( M \) is a section of the tangent bundle on \( M \). We denote the set of vector fields by \( \vfld M \).
\end{defn}

Now since the tangent bundle is an exponential object, we can can automatically establish a correspondence between three familiar conceptions of a vector field. On the first hand a vector field is a map
\begin{equation*}
  X:M\to M^D
\end{equation*}
such that \( X(p)(0)=p \) for every \( p\in M \). But then \( X \) is equivalent to giving a map
\begin{equation*}
  X:M\times D \to M
\end{equation*}

such that \( X(p,0)=p \). Thus, the notion of infinitesimal flow is recovered. Furthermore we have the principal of superposition.

\begin{proposition}
Let \( X:M\times D\to M \) be a vector field. Then for every \( p\in M \) and \( (d_1,d_2)\in D(2) \)
\begin{equation*}
  X(p,d_1+d_2) = X(X(p,d_1),d_2)
\end{equation*}
\label{prop:superpos}
\end{proposition}

\begin{proof}
  By fixing \( p \) we obtain two maps \( f,g:D(2)\to M \), which are defined by
  \begin{align*}
    f(d_1,d_2) &= X(p,d_1+d_2)    \\
    g(d_1,d_2) &= X(X(p,d_1),d_2) \\
  \end{align*}
  Since \( X \) is a vector field we have the equalities
  \begin{align*}
    f(d,0) &= g(d,0) \\
    f(0,d) &= g(0,d) \\
  \end{align*}
  hence, since \( M \) is microlinear, \( f \) and \( g \) are the same map.
\end{proof}

Lastly, we also have the notion of a vector field as an ``infinitesimal transformation of the identity''. That is, from a map
\begin{equation*}
  X:D\times M\to M
\end{equation*}
there corresponds a map (which we'll call by the same name)
\begin{equation}
  X:D\to M^M
  \label{eq:vecfield3}
\end{equation}
%TODO: make sure not all flows are diffeo
and it is such that \( X(0)=\id_M \).

From here on we will use the three ways of giving a vector field interchangeably. As in \cite{lav96} we refer to the image of an element \( d\in D \) by a map such as \ref{eq:vecfield3}, as \( X_d:M\to M \). For \( p\in M \), we should vizualize \( X_d(p) \) as the result of letting \( p \) flow along the vector field for \( d \) units of ``time''. Whereas in classical differential geometry the flow corresponding to a vector field need not define a group of transformations (but it does a semi-group), it will be the case here.

\begin{proposition}
  Let \( X:M\to M \) be a vector field. Then for all \( d\in D \), \( X_d \) is a bijection and \( X_d^{-1}=X_{-d} \).
  \label{prop:inftrans}
\end{proposition}

\begin{proof}
  This is a simple consequence of proposition \ref{prop:superpos}, since
  \begin{equation*}
    X(X(p,d),-d)=X(p,0)=p
  \end{equation*}
  for all \( p\in M \).
\end{proof}

The following proposition also establishes a strong parallel with classical Lie algebras of vector fields (although the ``Lie'' part will be seen later).

\begin{proposition}
  Let \( M \) be microlinear. Denote the set of invertible maps from \( M \) to itself as \( \aut(M) \). This is a microlinear object, and \( \vfld M \) is \( R \)-module isomorphic to \( T_{\id_M}\aut(M) \), the tangent space at the identity.
\end{proposition}

\begin{proof}
  To prove that \( \aut(M) \) is microlinear, we proceed as usual to prove that it is a limit of microlinear objects. To do this, note that \( \aut(M) \) is equal to the set
  \begin{equation*}
    \{ (f,g) \in M^M\times M^M \mid g\circ f = f \circ g = \id_M \}
  \end{equation*}

  In other words, we have a diagram
  \begin{equation*}
    \xymatrix{
      {\aut(M)} \ar@{>->}[r]^{i} & M^M\times M^M \ar@<1ex>[r]^{c_1} \ar[r]^{c_2}\ar@<-1ex>[r]^{I} & M^M
    }
  \end{equation*}
  where \( i \) is the mapping \( f\mapsto (f,f^{-1}) \), \( c_1 \) is the mapping \( (f,g)\mapsto f\circ g \), \( c_2 \) is \( (f,g)\mapsto g\circ f \), and \( I \) is \( (f,g)\mapsto \id_M \). The diagram commutes, and any object mapping into \( M^M\times M^M \) in such a way will clearly be in bijection with (a subset of) \( \aut(M) \). And, of course, \( M^M\times M^M \) along with \( M^M \) are microlinear, as per proposition \ref{prop:expmicro}. Now, any element \( t \) of \( T_{\id_M}\aut(M) \) is clearly associated with an element of \( T_{\id_M}(M^M) \), namely the ``same'' map, extending the codomain. Thus, it is an element of \( \vfld M \), thanks to the last interpretation of vector fields we observed above. Conversely, by proposition \ref{prop:inftrans}, any element of \( \vfld M \) maps \( D \) to invertible maps in \( M^M \), in other words elements of \( \aut(M) \). In summary, the map \( \vfld M \to T_{\id_M}\aut(M) \) given by
  \begin{equation*}
    (t:D\to M^M) \mapsto (\bar t:D\to \aut(M))
  \end{equation*}
  is a bijection. It is trivially homogeneous (\( \lambda t \mapsto \lambda \bar t \)), which means it is linear, thanks to proposition \ref{prop:homg}.
\end{proof}

\section{The Lie algebra of vector fields}

In classical differential geometry, the Lie bracket of two vector fields is often thought of a commutator of the ``flows'' induced by the vector fields (and in the case of matrix Lie groups the Lie bracket is indeed represented by matrix commutators). The ``problem'', as noted earlier, is that classical flows may not correspond to permutations of the manifold (again, in general only a semi-group of transformations). That isn't to say that differential geometers suffer greatly from this, but we again take the opportunity to show how synthetic differential geometry actually behaves how we would like to \emph{think} about geometry behaving. That is, the Lie bracket will be directly associated to the commutator of the bijections defined by their flows.

Let \( X,Y \) be vector fields on \( M \). They each define bijections \( X_d,Y_d:M\to M \) for all \( d\in D \). Let \( d_1,d_2\in D \), and consider the commutator of \( X_{d_1}, Y_{d_2} \) as a map \( \tau:D\times D \to M^M \):
\begin{equation*}
  \tau(d_1,d_2) = Y_{-d_2}\circ X_{-d_1}\circ Y_{d_2}\circ X_{d_1}
\end{equation*}

Let us recall lemma \ref{lm:3coequ}, which, in summary, states that for any \( g:D\times D \to M \) such that

\begin{equation*}
g(d_1,0) = g(0,d_2) = g(0,0) \quad \forall d_1,d_2\in D
\end{equation*}

there exists a unique map \( t:D\to M \) with the property

\begin{equation*}
  g(d_1,d_2) = t(d_1d_2)
\end{equation*}

The same conclusion applies when instead we have \( g:D\times D\to M^M \), as the only property of \( M \) used in the proof is that it is microlinear. It will be the case that \( \tau \) satisfies those conditions:

\begin{align*}
  \tau(d_1,0) &= Y_0\circ X_{-d_1}\circ Y_0\circ X_{d_1} \\
              &= X_{-d_1}\circ X_{d_1}                   \\
	      &= \id_M = \tau(0,0)                       \\
  \tau(0,d_2) &= Y_{-d_2}\circ X_0\circ Y_{d_2}\circ X_0 \\
              &= Y_{-d_2}\circ Y_{d_2}                   \\
	      &= \id_M = \tau(0,0)                       \\
\end{align*}

Therefore there exists a unique \( t:D\to M \) with \( \tau(d_1,d_2) = t(d_1d_2) \) for all \( d_1,d_2 \). This allows us make the following

\begin{defn}
  Let \( X,Y \) be vector fields on \( M \) (a microlinear object). The \emph{Lie bracket} of \( X \) and \( Y \) is the unique vector field, denoted \( [X,Y] \), such that
  \begin{equation*}
    [X,Y](d_1,d_2) = Y_{-d_2}\circ X_{-d_1}\circ Y_{d_2}\circ X_{d_1}
  \end{equation*}
\end{defn}

What we'll be pleased to find is that this bracket is bilinear, antisymmetric, and it satisfies the Jacobi identity. In short:

\begin{proposition}
  The pair \( (\vfld M, [\farg,\farg]) \) is a Lie algebra.
\end{proposition}

\begin{proof}
  \begin{itemize}
    \item The Lie bracket is antisymmetric. We only need to verify that \( [X,Y](d) = -[Y,X](d) \) for \( d = d_1d_2 \), for some \( d_1,d_2\in D \), by virtue of the uniqueness property of \( [\farg,\farg] \).
      \begin{align*}
	[X,Y]_{d_1d_2} &= [X,Y]_{-(-d_1d_2)}                                                   \\
		       &= \left( [X,Y]_{-d_1d_2} \right)^{-1}                                  \\
		       &= \left( Y_{-d_2}\circ X_{d_1}\circ Y_{d_2}\circ X_{-d_1} \right)^{-1} \\
		       &= X_{d_1}\circ X_{-d_2}\circ X_{-d_1}\circ Y_{d_2}                     \\
		       &= [Y,X]_{d_2(-d_1)}                                                    \\
		       &= \left( -[Y,X] \right)_{d_1d_2}                                       \\
      \end{align*}
    \item Again we utilize that \( \vfld M \) is Euclidean to prove linearity, by only proving homogeneity (prop. \ref{prop:homg}). Let \( \lambda\in R \). Then
      \begin{align*}
	\lambda[X,Y]_{d_1d_2} &= [X,Y]_{\lambda d_1d_2}                                               \\
	                      &= Y_{-d_2}\circ X_{-\lambda d_1}\circ Y_{d_2}\circ X_{\lambda d_1}     \\
			      &= Y_{-d_2}\circ (\lambda X)_{-d_1}\circ Y_{d_2}\circ (\lambda X)_{d_1} \\
			      &= [\lambda X,Y]_{d_1d_2}
      \end{align*}
    \item It remains to prove the Jacobi identity:
      \begin{equation*}
	[[X,Y],Z] + [[Y,Z],X] + [[Z,X],Y] = 0
      \end{equation*}
      for all vector fields \( X,Y,Z \). We will not prove it here, but the idea is the same as in the previous proofs. First note that it suffices to prove it on elements \( d_1d_2d_3 \). You then expand the left hand side to
      \begin{align*}
	&Z_{-d_3}\circ X_{-d_1}\circ Y_{-d_2}\circ X_{d_1}\circ Y_{d_2}\circ Z_{d_3}\circ Y_{-d_2}\circ X_{-d_1}\circ Y_{d_2}\circ X_{d_1} \\
        &\circ X_{-d_1}\circ Y_{-d_2}\circ Z_{-d_3}\circ Y_{d_2}\circ Z_{d_3}\circ X_{d_1}\circ Z_{-d_3}\circ Y_{-d_2}\circ Z_{d_3}\circ Y_{d_2} \\
        &\circ Y_{-d_2}\circ Z_{-d_3}\circ X_{-d_1}\circ Z_{d_3}\circ X_{d_1}\circ Y_{d_2}\circ X_{-d_1}\circ Z_{-d_3}\circ X_{d_1}\circ Z_{d_3} 
      \end{align*}
      and make a series of judicious manipulations to reach \( 0(d_1d_2d_3 = \id_M \). The complete proof can be found in \cite{lav96}.
  \end{itemize}
\end{proof}

\section{Derivations}
 The final construction we'll look at is that of the Lie derivative associated with a vector field. Its definition is much the same as the classical one; we evaluate functions \( M\to R \) along the flow induced by a vector field. Recall from the previous section that a vector field \( X \) is equivalent to giving a function
 \begin{equation*}
   X:M\times D\to M
 \end{equation*}

Hence, given a function \( f:M\to R \) the mapping
\begin{equation*}
  f(X(p,\farg):D\to R
\end{equation*}
is determined uniquely by \( f(X(p,0)) = f(p) \), and some \( b\in R \) with
\begin{equation*}
  f(X(p,d)) = f(p) + b(p)d
\end{equation*}
Here \( p\in M \) is fixed, and \( b \) depends on it (and \( f \), and \( X \)), as we've made explicit. Varying \( p \) gives a function \( M\to R \), leading to this

\begin{defn}
  Let \( f:M\to R \) be a function, and \( X\in \vfld M \). The Lie derivative of \( f \) along \( X \) is the unique function
  \begin{equation*}
    \lie_Xf:M\to R
  \end{equation*}
  such that
  \begin{equation*}
    f(X(p,d)) = f(p) + d\cdot(\lie_Xf)(p)
  \end{equation*}
\end{defn}

The Lie derivative is a derivation, as we will see shortly. We should call to memory that a \emph{derivation} of an algebra \( A \) is a linear map \( D:A\to A \) such that \( D(fg) = D(f)g + fD(g) \) for all \( f,g\in A \).

\begin{proposition}
  The map given by associating each function \( f:M\to R \) to its Lie derivative along \( X \) is a derivation of \( R \)-algebras.
\end{proposition}

\begin{proof}
  Homogeneity, and therefore linearity, is a given, since
  \begin{align*}
    (\lambda\lie_Xf)(p)    &= \lambda f(p) + d\cdot\lambda(\lie_Xf)(p)      \\
    (\lie_X(\lambda f))(p) &= (\lambda f)(p) + d\cdot(\lie_X(\lambda f))(p) \\
                           &= \lambda f(p) + d\cdot(\lie_X(\lambda f))(p)
  \end{align*}
  and by uniqueness \( \cdot\lambda(\lie_Xf)(p) = d\cdot(\lie_X(\lambda f))(p) \). The ``product rule'' will follow from the same proof we gave of the calculus version in section \ref{sec:basictheory}. Let \( f,g:M\to R \) be functions. The Lie derivative along \( X \) is defined by the expression
  \begin{align*}
    (fg)(X(p,d)) &= f(X(p,d))g(X(p,d))                                     \\
                 &= (f(p) + d\cdot(\lie_Xf)(p))(g(p) + d\cdot(\lie_Xg)(p)) \\
		 &= f(p)g(p) + d((\lie_Xf)(p)g(p) + f(p)(\lie_Xg)(p))
  \end{align*}
  and the unique coefficient multiplying \( d \) is clearly the same as for \( \lie_Xf\cdot g + f\cdot\lie_Xg \).
\end{proof}<++>
