\section{The basic theory}

The axiomatic theory of Synthetic Differential Geometry begins by assuming a certain topos \( \sdgE \) exists. A topos is a category with certain properties which are meant to abstract the way in which the category of sets behaves. In fact, most naive set theoretic and logical notions have meaningful interpretation in terms of objects and morphisms of a topos. Thus, from this point on we will mainly omit the fact that we are working in a topos, and use standard set theoretic notation, giving only brief reminders now and then. 

\subsection{Axiomatics}

We begin with an object \( R \). We can think of \( R \) as an extension of the notion of the standard continuum, \( \mathbb{R} \). Thus \( R \) should be a commutative ring with unit. However, due to an axiom that we will shortly introduce, we cannot ask that \( R \) be a field. Later on, we will need to strengthen this requirement to, for example ``2:=1+1 is invertible''. But later on we may need that ``3 is invertible''. Instead of introducing these small axioms successively, we will take care of them succintly with our:

\begin{axiom}
  \label{ax0}
  \( R \) is a \( \Q \)-algebra.
\end{axiom}

With that out of the way, we proceed to introduce a central object of study, the ``infinitesimals''. These come in the form of nilpotent elements of \( R \). To be precise, let \( D = \{d\in R \mid d^2=0\} \). The (first version of the) defining axiom of SDG concerns \( D \), and as in all the literature we refer to it as the Kock-Lawvere axiom.

\begin{klaxiom}
  \label{KL1}
  For all \( f:D\to R \) there exist unique \( a,b\in R \) such that \( f(d) = a + bd\,\forall d\in D \)
\end{klaxiom}

Note that, in particular, \( a=f(0) \). Of course, this is a strong requirement. So strong in fact, that we will have to weaken our logic in order for the theory to be remotely interesting. This is because of the following proposition.

\begin{proposition}
  \label{0prop}
  \( R=\{0\} \)
\end{proposition}

\begin{proof}
  Let \( f:D\to R \) be defined by
  \begin{equation*}
    f(d) =
    \left\{ 
      \begin{aligned}
	1,& & d \neq 0 \\
	0,& & d = 0
      \end{aligned}
    \right.
  \end{equation*}
By the KL axiom \ref{KL1}, there exist unique \( a,b\in R \) such that \( f(d) = a + bd \) for all \( d \). Since \( a =  f(0) = 0 \), there exists \( b\in R \) such that \( f(d) = bd \) for all \( d \). Thus for non-zero \( d \) we have \( 1 = bd \). Squaring both sides yields \( 1 = 0 \), and since \( R \) is a ring this concludes the proof.
\end{proof}

However, the above proof relies on the ``fact'' that, for any \( x\in R \) we have either \( x = 0 \) or \( x\neq0 \). Indeed the well-definedness of the function \( f \) depends on this assertion. This is true in classical logic, as it is a simple application of the \textbf{Law of the Excluded Middle} (LEM):
\begin{equation*}
  \forall p\, p\vee \neg p
\end{equation*}
%TODO: comment about internal logic of topos is intuitionistic
To carry on with our theory, we see that we have to reject LEM. The above proof then fails, but of course just because of that we cannot conclude that the theory functions properly. To do so would by all means require exhibiting the category \( \sdgE \) explicitly. For now we will continue ignoring this, and develop the naive theory, knowing that we have to abstain from assuming LEM.

Without LEM, (\ref{0prop}) of course still contradicts (because we do want R to be a \( \Q \)-algebra) the statement
\begin{equation}
  \forall d\in D\,(d=0) \vee \neg (d=0)
  \label{eq:not0}
\end{equation}
In other words, its negation is true. One may be inclined to ``simplify'' the negation statement, leading to the apparent absurdity that
\begin{equation*}
  \exists d\in D\, \neg(d=0) \wedge \neg\neg (d=0)
\end{equation*}
Note that we haven't eliminated the double negation, since \( \neg\neg p\rightarrow p \) is equivalent to LEM. Still, the above would be a contradiction, since \( p\wedge \neg p \) is still false in intuitionistic logic. But, the first equivalence we used on the universally quantified statement (\ref{eq:not0}) also fails to be valid in intuitionistic logic (see \cite[p. 248]{fra84} for example).

%TODO: find a better proof of this
So what are the elements of \( D \)? For one they are not all zero, for if they were then every function \( d\mapsto ad, a\in R \) would be equal to the constant \( 0 \), but since each coefficient \( a \) is unique, we again reach the absurdity that every \( a\in R \) is the same (equal to zero). At the same time the negation of (\ref{0prop}) implies that not all \( d \) that aren't zero satisfy \( \neg(d=0) \). The consequence is the simple statement that
\begin{equation*}
  \exists d\in D \neg\neg(d=0)
\end{equation*}

The infinitesimals \( D \) are thus neither zero nor nonzero. This is a contradictory statement in classical logic, but it is precisely of the type of phrases that we should expect to appear if we weaken our logic to intuitionistic logic.

%TODO: cancellation of universally quantified d
\subsection{Elementary Calculus}

With our basic axiom scheme in place we can begin revisiting the classical notion of a derivative in this new context. We begin, as expected, with a function \( f:R\to R \). Fix \( x\in R \) and define a new function \( g:D\to R \) by
\begin{equation*}
  d\mapsto f(x+d)
\end{equation*}
By the KL axiom \ref{KL1} there are unique \( a,b\in R \) such that
\begin{equation*}
  \forall d\in D\, f(x+d) = f(x)+bd
\end{equation*}
(since \( g(0) = f(x) \)). This naturally leads to the following
\begin{defn}
  Let \( f:R\to R \). The \emph{derivative} of \( f \) at \( x \), denoted \( f'(x) \), is the unique \( b\in R \) such that \( f(x+d)=f(x)+bd\,\forall d\in D \).
  \label{def:1varder}
\end{defn}

Clearly this also allows us to define the derivative \emph{function} \( f':R\to R \). For instance, let us calculate the derivative of a simple polynomial function. Let \( f:x\mapsto x^2+x \) then
\begin{align*}
  f(x+d) & =  x^2+2xd + d^2 + x + d \\ 
         & =  x^2 + x + (2x+1)d
\end{align*}
by which \( f'(x)=2x+1 \). So far our ``formal'' calculus agrees with standard calculus. In fact, we have the following proposition:

\begin{proposition}
  Let \( f,g:R\to R \) and let \(\alpha\in R.\) The following hold:
  \begin{enumerate}
    \item \((f+g)' = f'+g'\)
    \item \((\alpha f)' = \alpha f'\)
    \item \((fg)' = f'g + fg'\)
  \end{enumerate}
\end{proposition}
\begin{proof}
  The proof is as simple as one could suspect, here we only prove the third identity as an example. We have:
  \begin{align*}
    f(x+d)g(x+d) & = (f(x)+f'(x)d)(g(x)+g'(x)d) \\
                 & = f(x)g(x) + (f'(x)g(x)+f(x)g'(x))d + f'(x)g'(x)d^2 \\
		 & = f(x)g(x) + (f'(x)g(x)+f(x)g'(x))d  
    \label{eq:prodrule}
  \end{align*}
  At a given \( x\in R \), this clearly holds for all \( d\in D \), giving us the result.
\end{proof}

Furthermore we have the Chain Rule:
\begin{proposition}
  Let \( f,g:R\to R \) be two functions. Then the derivative of the composite \( g\circ f \) is
  \begin{equation*}
    (f\circ g) = (f'\circ g)\cdot g'
  \end{equation*}
\end{proposition}
\begin{proof}
  Let \( x\in R \). Then
  \begin{equation*}
   (f\circ g)(x+d) = f(g(x)) + (f\circ g)'(x)d
  \end{equation*}
  by definition. Meanwhile, if we expand \( g(x+d) \) first:
  \begin{align*}
    (f\circ g)(x+d) & = f(g(x+d)) \\
                    & = f(g(x)+g'(x)d) \\
  \end{align*}
  Observe that \( g'(x)d\in D \), since \( (g'(x)d)^2 = g'(x)^2d^2 = 0 \). Now, \( f'(g(x)) \) is the unique coefficient satisfying
  \begin{equation*}
    f(g(x)+g'(x)d) = f(g(x)) + f'(g(x))g'(x)d
  \end{equation*}
\end{proof}

The reader may notice that these statements seem lazily worded. Normally, we would include in the hypotheses something like ``\textit{Let \( f,g:R\to R \) be} differentiable \textit{functions. Then} (some other function) \textit{is differentiable, and \dots etc.}'' We are not in fact forgetting hypotheses. The Kock-Lawvere Axiom \ref{KL1} actually implies that \textit{every} function from \( R \) to \( R \) is differentiable. This is easy to verify by noting that in the definition of the derivative (\ref{def:1varder}), no additional assumptions are made other than \( f \) being a function. As a direct consequence, every function from \( R \) to \( R \) is infinitely differentiable.

This leads us to ask if we also have Taylor expansions of functions. In classical calculus, a Taylor expansion of second order involves a second degree polynomial, and ``terms of order 3''. In our context, these are simply nilcube elements, i.e. \( \delta\in R : \delta^3 = 0 \). An example of such a nilcube is any \( d_1+d_2 \) where \( d_1,d_2 \) are both in \( D \). In effect,
\begin{equation*}
  (d_1+d_2)^3 = d_1^3 + d_2^3 + 3d_1^2d_2 + 3d_1d_2^2 = 0
\end{equation*}

Furthermore, if \( f:R\to R\) is a function, 
\begin{align*}
  f(x+d_1+d_2) & = f(x+d_1)+f'(x+d_1)d_2 \\
               & = f(x) + f'(x)d_1 + f'(x)d_2 + f''(x)d_1d_2
\end{align*}
and meanwhile,
\begin{align*}
  (d_1+d_2)^2 & = 2d_1d_2
\end{align*}
Joining the two together yields:
\begin{equation*}
  f(x+d_1+d_2) = f(x) + f'(x)(d_1+d_2) + \frac{f''(x)}{2}(d_1+d_2)^2
\end{equation*}

What we have just proven is the following proposition.
\begin{proposition}
  If \( f:R\to R \) is a function and \( d_1,d_2\in D \), then \( \delta = d_1+d_2 \) satisfies
  \begin{equation*}
    f(x+\delta) = f(x) + f'(x)\delta + \frac{f''(x)}{2}\delta^2
  \end{equation*}

\end{proposition}

This justifies the earlier heuristic of considering nilcubes as candidates for second order Taylor expansions. Unfortunately, there is no way to assert as of now that \textit{any} nilcube must be of the form \( d_1+d_2 \) with \( d_1,d_2\in D \). It is a partial result. In section {/placeholder1/} we will introduce Taylor expansions for every nilpotent element by way of an axiom that generalizes the KL axiom \ref{KL1}. Still, it serves as a nice exercise to prove the following, more general, Taylor formula.

\begin{proposition}
  Let \( f:R\to R \) be a function. Then for all \( n\in\mathbb N \) and \( \delta = d_1+\cdots+d_n \)
  \begin{equation*}
    f(x+\delta) = f(x) + f'(x)\delta + \frac{f''(x)}{2!}\delta^2 + \cdots + \frac{f^{(n)}}{n!}\delta^n
  \end{equation*}
  where \( x\in R \), \( d_1,\dots,d_n\in D \)
\end{proposition}

\begin{proof}
  To begin with, let us look at the powers of \( \delta \), that is, the usual multinomial formula:
  \begin{equation*}
    \delta^k = (d_1+\cdots+d_n)^k = \sum_{i_1+\cdots+i_n=k}\left( k\atop i_1,\dots,i_n \right)\prod_{r=1}^{n}d_r^{i_r}
  \end{equation*}
  Recall that the \( d_r \) are in \( D \), so only the only non-zero summands will be those with each \( d_r \) raised to either 1 or 0. That is, choosing a subset of the \( i_r \) to be 1, and the rest 0. In that case the multinomial coefficients are simply \( k! \), giving us
  \begin{equation*}
    \delta^k = \sum_{I\subset \{1,\dots,n\}\atop \abs{I}=k}k!\prod_{t\in I}d_t
  \end{equation*}
  in other words, the \( k \)-\textit{th} elementary symmetric polynomial of \( n \) variables (multiplied by \( k! \)). Denote this by \( e_k(X_1,\dots,X_n) \), and convene \( e_0 \equiv 1 \) and \( e_k(X_1,\dots,X_n) = 0 \) if \( k>n \). We proceed inductively:
  % [TODO] Fix this equation
  \begin{align}
    \begin{split}
      f(x+\delta) & = f(x+d_1+\cdots+d_n) \\[10pt]
                  & = f(x+d_1+\cdots+d_{n-1})+f'(x+d_1+\cdots+d_{n-1})d_n \\[10pt]
		  & = \sum_{i=0}^{n-1}\frac{f^{(i)}(x)}{i!}i!e_i(d_1,\dots,d_{n-1}) + d_n\sum_{i=0}^{n-1}\frac{f^{(i+1)}(x)}{i!}i!e_i(d_1,\dots,d_{n-1}) \\[10pt]
                  & = \sum_{i=0}^{n-1}f^{(i)}(x)\left( e_i(d_1,\dots,d_{n-1})+d_ke_{i-1}(d_1,\dots,d_{n-1})\right) + d_kf^{(n)}(x)e_{k-1}(d_1,\dots,d_{k-1})
    \end{split}
    \label{eq:ntaylor}
  \end{align}
It's now useful to use the following recursion that elementary symmetric polynomials satisfy:
\begin{equation*}
  e_k(X_1,\dots,X_n) = e_{k}(X_1,\dots,X_{n-1}) + X_{n}e_{k-1}(X_1,\dots,X_{n-1})
\end{equation*}
Finally, observe that \( e_{k-1}(d_1,\dots,d_{k-1}) \) is just \( d_1d_2\cdots d_{k-1} \), meaning that \( d_ke_{k-1}(d_1,\dots,d_{k-1}) = e_k(d_1,\dots, d_{k-1},d_k) \). With these two identities, \ref{eq:ntaylor} becomes:
\begin{equation*}
  \sum_{i=0}^{n}f^{(i)}(x)e_i(d_1,\dots,d_k)= \sum_{i=0}^n \frac{f^{(i)}(x)}{i!}\delta^j
\end{equation*}
\end{proof}

We'd do well to note that, for \( n\geq k+1 \), \( \delta^n \) is 0. The above Taylor formula is of course still valid for any \( k \) and \( n \), but the terms are all zero after \( k+1 \). We again find a unique characterization of all functions on a certain set of nilpotents. This property will be stated in section {/placeholder1/} as a general axiom.

\subsection{Calculus of several variables}
We now take the usual course in calculus, which is to generalize the previous study to functions of more than one variable, i.e. \( R^n\to R \), and further on functions \( E\to V \) where \( E \) and \( V \) are ``vector spaces'' in a sense that will be made precise.

\subsubsection{Scalar functions}
Let \( f:R^n\to R \) be a function, and \( r=(r_1,\dots,r_n)\in R^n \). As one would expect, we calculate the partial derivatives by adding an infinitesimal increment along a single coordinate direction. That is, let \( g: D\to R \) be defined by
\begin{equation*}
  d\mapsto f(r_1+d,\dots,r_n)
\end{equation*}
By the KL axiom \ref{KL1}, there exists a unique \( b\in R \) such that
\begin{equation*}
  f(r_1+d,\dots,r_n) = f(r) + bd
\end{equation*}
We define \( \ddx{f}{x_1}(r_1,\dots,r_n)\) as \( b \), which simultaneaously defines a function \( \ddx{f}{x_1}:R^n\to R \) Similarly, we define \( \ddx{f}{x_2},\dots,\ddx{f}{x_n} \). By iterating the process we obtain higher partial derivatives, denoted
\begin{equation*}
  \ddxk{k}{f}{x_{i_1}}{x_{i_k}}
\end{equation*}

A nice result is a parallel of Schwarz' theorem of the interchangeability of partial derivatives.

\begin{proposition}
  Let \( f:R^n\to R \) be a function. Then for any \( 1\leq i,j\leq n \)
  \begin{equation*}
    \ddxII{f}{x_i}{x_j}= \ddxII{f}{x_j}{x_i}
  \end{equation*}
  \label{prop:schwarz}
\end{proposition}
%TODO: this part looks horrible
The previous result follows directly from the following lemma which generalizes the second-order Taylor expansion to two dimensions.
\begin{lemma}
  Let \( f:R^n \to R \) be a function. Then for all \( (d_1,d_2)\in D\times D \)
  \begin{align*}
    f(r_1,\dots,r_i+d_1,\dots,r_j+d_2,\dots,r_n) = f(x_1,\dots,x_i,\dots,x_j+d_2,\dots,x_n) + \\
    \qquad + \ddx{f}{x_i}(x_1,\dots,x_i,\dots,x_j,\dots,x_n)d_1 + \ddxII{f}{x_i}{x_j}(x_1,\dots,x_n)d_2d_1 
  \end{align*}
\end{lemma}

\begin{proof}
  Apply the definition of the partial derivative in \( x_i \), and succesively in \( x_j \)
  \begin{align*}
    f(r_1,\dots,r_i+d_1,\dots,r_j+d_2,\dots,r_n) & = f(x_1,\dots,x_i,\dots,x_j+d_2,\dots,x_n) + \\
                                                 & \qquad\qquad + \ddx{f}{x_i}(x_1,\dots,x_i,\dots,x_j+d_2,\dots,x_n)d_1 \\
                                                 & = f(x_1,\dots,x_n) + \ddx{f}{x_i}(x_1,\dots,x_n)d_1 + \ddxII{f}{x_i}{x_j}(x_1,\dots,x_n)d_2d_1
  \end{align*}
  and \( f(x_1,\dots,x_n), \ddx{f}{x_i}(x_1,\dots,x_n), \ddx{f}{x_j}(x_1,\dots,x_n), \ddxII{f}{x_i}{x_j}(x_1,\dots,x_n)  \) are the unique coefficients that satisfy this for universally quantified \( (d_1,d_2)\in D \).
\end{proof}

On the other hand, exchanging \( d_1 \) and \( d_2 \) (and commuting products and sums) in the above equations yields the same conclusion for \( \ddxII{f}{x_j}{x_i}(x_1,\dots,x_n) \). Since they are unique they must equal each other, proving (\ref{prop:schwarz}). By induction we also obtain that the order in which we differentiate higher partial derivatives does not matter.

\subsection{Vector functions}
We will now examine functions between \( R \)-modules. However, for a general \( R \)-module \( V \), it does not follow from the KL axiom (\ref{KL1}) that a version of it holds for functions \( D\to V \). In this case what we do is simply restrict our attention to those \( R \)-modules where this is the case. This leads to the following definition:

%TODO: example of R-module that isn't Euclidean.
\begin{defn}
  An \( R \)-module \( V \) is said to be a \emph{Euclidean \( R \)-module} if the vector form of the KL axiom holds. That is, for any function \( f: D\to V \),
  \begin{equation*}
    \exists! \, \vec a,\vec b\in V \text{ such that } f(d) = \vec a + d\vec b \,\forall d\in D
  \end{equation*}
  (again \( \vec a \) is evidently \( f(0) \))
\end{defn}

This may seem like a very ad-hoc definition, but we will encounter many Euclidean \( R \)-modules naturally. For example we have the following lemma.

\begin{lemma}
  \leavevmode
  \begin{enumerate}
    \item \( R^n \) is a Euclidean \( R \)-module.
    \item For any object \( X \), and Euclidean \( R \)-module \( V \), the exponential \( V^X \) is a Euclidean \( R \)-module. 
  \end{enumerate}
  \label{lm:Emod}
\end{lemma}

\begin{proof}
  \leavevmode
  \begin{enumerate}
    \item This is evident since a function \( f:D\to R^n \) is uniquely determined by each of its component functions. We let \( f_i = \pi_i \circ{f}  \) where \( \pi_i:R^n\to R \) is the \( i \)-\textit{th} coordinate projection. Then \( \vec a=(a_1,\dots,a_n), \vec b=(b_1,\dots,b_n) \) are the unique coefficients in the claim. Here each \( a_i,b_i \) is obtained by applying the KL axiom to \( f_i \).
    \item First of all, \( V^X \) is an \( R \)-module by the pointwise sum of maps. Let \( f:D\to V^X \) be a function. This defines, for each \( x\in X \), a function \( g(x):D\to V \) by
    \begin{equation*}
      d\mapsto f(d)(x)
    \end{equation*}
    since \( V \) is Euclidean, there exist unique \( a(x), b(x) \) such that
    \begin{equation*}
      f(d)(x) = \vec a(x) + d\vec b(x)\,\forall d\in D
    \end{equation*}
    Evidently, varying \( x \) defines functions \( \vec a,\vec b:X\to V \), which are unique such that \( f(d) = \vec a + d\vec b \,\forall d\in D\)
  \end{enumerate}
\end{proof}

Additionally, it's clear that any \( V \) which is isomorphic to \( R^n \) as an \( R \)-module is too Euclidean. In \cite{kock10} for example, it begins directly with this notion, calling them ``finite dimensional vector spaces''. Another fundamental example of a Euclidean \( R \)-module that we'll study are tangent spaces, in section {/placeholder2/}.

For now let us go back to doing calculus. The Euclidean structure allows one to define directional derivatives, as is done in \cite{lav96}, for example.
\begin{defn}
  Let \( V,E \) be Euclidean \( R \)-modules, \( f:V\to E \) a function and \( \vec u, \vec a\in V \). The derivative of \( f \) at \( \vec a \) in direction \( \vec u \) is the unique \( \vec b\in E \) such that
  \begin{equation*}
    \forall d\in D,\, f(\vec a + d\vec u) = f(\vec a) + d\vec b
  \end{equation*}
  We denote this by \( \partial_{\vec u}f(a) \). 
\end{defn}

We note that \( \partial_{(\farg)}f(\vec a) \) is a linear function. In other words, for all \( \lambda,\mu \in R\),
\begin{equation*}
  \partial_{\lambda \vec u + \mu \vec v}f(\vec a) = \lambda\partial_{\vec u}f(\vec a) + \mu\partial_{\vec v}f(\vec a)
\end{equation*}
The proof is a straightforward excercise (one may consult \cite[p. 13]{lav96}).

If \( V \) is finitely generated, i.e. \( V = \spn{\{e_1,\dots,e_n\}} \), then one can define
\begin{equation*}
  \ddx{f}{x_i}(\vec a) = \partial_{e_1}f(\vec a)
\end{equation*}
and if \( \vec u = \sum_{i=1}^{n}{u_ie_i} \) then by linearity
\begin{equation*}
  \partial_{\vec u}f(\vec a) = \sum_{i=1}^{n}\ddx{f}{x_i}u_i
\end{equation*}

Finally, the familiar total differential makes an appearance here too.

\begin{defn}
  Let \( f:V\to E \) be a function. The \emph{differential} of \( f \) at \( \vec a\in V \) is the function \( df(\vec a):V\to E \) defined by
  \begin{equation*}
    \vec u\mapsto \partial_{\vec u}f(\vec a)
  \end{equation*}
  \label{def:df(a)}
\end{defn}

As noted before, \( df(\vec a) \) defines a linear map \( V\to E \), for each \( a\in V \). It will also be useful to prove that, if \( f \) is linear, it is equal to its own differential. First we need a

\begin{lemma}
  Let \( V \) be an \( R \)-module, and \( E \) a Euclidean \( R \)-module. Let \( f:V\to E \) be such that
  \begin{equation*}
    f(\lambda \vec a) = \lambda f(\vec a)
  \end{equation*}
  for all \( \vec a\in V \) and \( \lambda \) in \( R \) (f is homogeneous). Then \( \forall \vec a\in V,\, df(\vec a) = df(\vec 0) \).
  \label{lm:homg}
\end{lemma}

\begin{proof}
  Let \( \vec a\in V \). Since \( E \) is Euclidean, we have
  \begin{equation*}
    f(\vec x + d\vec u) = f(\vec x) + d\partial_{\vec u}f(\vec x)
  \end{equation*}
  for all \( d\in D \) and \( \vec x,u\in V \). Letting \( x=\lambda a \) and \( d = \lambda\overline d \) for some \( \overline d\in D \), we find that
  \begin{equation*}
    f(\lambda\vec a + \lambda d\vec u) = f(\lambda\vec a)+\lambda d\partial_{\vec u}(\lambda\vec a)
  \end{equation*}
  At the same time, since \( f \) is homogeneous,
  \begin{align*}
    f(\lambda\vec a + \lambda d\vec u) &= \lambda f(\vec a + d\vec u) \\
                                       &= \lambda (f(\vec a) + d\partial_{\vec u}f(\vec a))
  \end{align*}
  Putting together the two equalities, we see that for all \( d\in D \),
  \begin{align*}
    \lambda d\partial_{\vec u}f(\lambda\vec a) = \lambda d\partial_{\vec u}f(\vec a) \Rightarrow \\
    \Rightarrow \lambda\partial_{u}f(\lambda\vec a) = \lambda\partial_{\vec u}(\vec a)
  \end{align*}
  Finally, we differentiate both sides with respect to \( \lambda \), which results in
  \begin{equation*}
    \partial_{\vec u}f(\lambda\vec a) + \lambda\frac{\partial}{\partial\lambda}(\partial_{\vec u}f(\lambda\vec a)) = \partial_{\vec u}f(\vec a)
  \end{equation*}
  Letting \( \lambda=0 \) concludes the proof.
\end{proof}

With that, we can quickly prove the following

\begin{proposition}
  Let \( V \) be an \( R \)-module, and \( E \) a Euclidean \( R \)-module. Let \( f:V\to E \) be homogeneous. Then \( f \) is linear and \( f = df(\vec 0) \).
  \label{prop:homg}
\end{proposition}

\begin{proof}
  Since \( E \) is Euclidean, for all \( \vec u\in V \) and \( d\in D \),
  \begin{align*}
    d\partial_{\vec u}f(\vec u) &= f(\vec u + d\vec u) - f(\vec u) \\
                                &= (1+d)f(\vec u) - f(\vec u)      \\
				&= d\cdot f(\vec u)
  \end{align*}
  by which
  \begin{equation*}
    f(\vec u) = \partial_{\vec u}f(\vec u) = \partial_{\vec u}(\vec 0) = df(\vec 0 )(\vec u)
  \end{equation*} 
\end{proof}
